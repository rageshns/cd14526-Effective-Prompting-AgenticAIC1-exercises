{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Lesson Exercise: Organize Your Workspace Space Kick-Off\n",
    "\n",
    "Before we get started on our course, let's take some time to set up and tidy our workspace. As an agentic twist on the matter, we will guide a Large Language Model (LLM) through a series of prompt refinements to create a practical plan to organize your personal work area.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Start with a generic prompt\n",
    "2. Add a professional role\n",
    "3. Introduce concrete constraints\n",
    "4. Bonus: Apply what you've learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# No changes needed in this cell\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the Vocareum API endpoint\n",
    "# TODO: Fill in the missing parts marked with **********\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    # Uncomment one of the following\n",
    "    # api_key=\"**********\",  # <--- TODO: Fill in your Vocareum API key here\n",
    "    # api_key=os.getenv(\n",
    "    #     \"OPENAI_API_KEY\"\n",
    "    # ),  # <-- Alternately, set as an environment variable\n",
    ")\n",
    "\n",
    "# If using OpenAI's API endpoint\n",
    "# client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No changes needed in this cell\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class OpenAIModels(str, Enum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
    "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "MODEL = OpenAIModels.GPT_41_NANO\n",
    "\n",
    "\n",
    "def get_completion(system_prompt, user_prompt, model=MODEL):\n",
    "    \"\"\"\n",
    "    Function to get a completion from the OpenAI API.\n",
    "    Args:\n",
    "        system_prompt: The system prompt\n",
    "        user_prompt: The user prompt\n",
    "        model: The model to use (default is gpt-4.1-mini)\n",
    "    Returns:\n",
    "        The completion text\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    if system_prompt is not None:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            *messages,\n",
    "        ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "def display_responses(*args):\n",
    "    \"\"\"Helper function to display responses as Markdown, horizontally.\"\"\"\n",
    "    markdown_string = \"<table><tr>\"\n",
    "    # Headers\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<th>System Prompt:<br />{arg['system_prompt']}<br /><br />\"\n",
    "        markdown_string += f\"User Prompt:<br />{arg['user_prompt']}</th>\"\n",
    "    markdown_string += \"</tr>\"\n",
    "    # Rows\n",
    "    markdown_string += \"<tr>\"\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<td>Response:<br />{arg['response']}</td>\"\n",
    "    markdown_string += \"</tr></table>\"\n",
    "    display(Markdown(markdown_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generic Prompt\n",
    "\n",
    "First, let's see what the model produces with a basic prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No changes needed in this cell\n",
    "plain_system_prompt = \"You are a helpful assistant.\"  # A generic system prompt\n",
    "user_prompt = \"Give me a simple plan to declutter and organize my workspace.\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "baseline_response = get_completion(plain_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": plain_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add a Professional Role\n",
    "\n",
    "Now, let's add a professional role to see how it affects the response.\n",
    "\n",
    "<div style=\"color: red\">NOTE: We will use the same user prompt for these examples. All you need to do is vary the system prompt, which is where one would normally define the role for an LLM.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a system prompt starting with \"You are...\" replacing the ***********\n",
    "role_system_prompt = \"***********\"\n",
    "\n",
    "print(\"Sending prompt with professional role...\")\n",
    "role_response = get_completion(role_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Show last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": plain_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": role_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": role_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Constraints\n",
    "\n",
    "Let's add specific constraints to see how the model prioritizes tasks. Let's add a time constraint, a budget constraint, and other constraints that are important for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a constraints system prompt replacing the ***********\n",
    "constraints_system_prompt = f\"\"\" {role_system_prompt}. ***********.\"\"\"\n",
    "\n",
    "print(\"Sending prompt with constraints...\")\n",
    "constraints_response = get_completion(constraints_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Show last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": role_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": role_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": constraints_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": constraints_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Note how the model:\n",
    "- Prioritizes tasks within the given timeframe\n",
    "- Works within the budget\n",
    "- Handles other constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Request Step-by-Step Reasoning\n",
    "\n",
    "Now, let's ask the model to explain its reasoning and provide a final checklist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ask the LLM to explain its reasoning step by step, replacing the ***********\n",
    "reasoning_system_prompt = (\n",
    "    f\"{constraints_system_prompt}. *********** before presenting the final checklist.\"\n",
    ")\n",
    "\n",
    "print(\"Sending prompt with reasoning request...\")\n",
    "reasoning_response = get_completion(reasoning_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Display the last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": constraints_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": constraints_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": reasoning_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": reasoning_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Examine the chain of thought for:\n",
    "- Clarity of reasoning\n",
    "- Logical progression, or lack thereof\n",
    "- How constraints may have affected decision-making\n",
    "- Additional insights provided by the reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reflection & Transfer\n",
    "\n",
    "Let's reflect on what we've learned from this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which prompt tweak changed the output most dramatically and why?\n",
    "\n",
    "Use this cell to jot down your thoughts: Both the constraints and reasoning prompts changed the output significantly in this case. Both of them had a plan at the beginning. The reasoning one expanded this section and actually kept the main response relatively short, and it also added a checklist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List two other situations where the same prompt-refinement pattern could help\n",
    "\n",
    "`TODO: Replace the ***********`\n",
    "1. `***********`\n",
    "2. `***********`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Apply what you've learned\n",
    "\n",
    "Try crafting a prompt for one of your own ideas or even a different organization task (e.g., digital file cleanup, closet overhaul)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own prompts here!\n",
    "\n",
    "# TODO: Replace the ***********\n",
    "custom_system_prompt = \"\"\"\n",
    "***********\n",
    "\"\"\"\n",
    "user_prompt = \"\"\"\n",
    "***********\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment the lines below to run your custom prompt\n",
    "print(\"Sending custom prompt...\")\n",
    "custom_response = get_completion(custom_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": custom_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": custom_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, we explored how different prompt refinements affect the output of an LLM:\n",
    "\n",
    "1. **Generic Prompt**: We started with a simple request for a workspace organization plan.\n",
    "2. **Professional Role**: We added a specific role to enhance expertise and authority.\n",
    "3. **Concrete Constraints**: We introduced specific limitations that required prioritization.\n",
    "4. **Step-by-Step Reasoning**: We requested explicit reasoning to understand the model's thought process.\n",
    "\n",
    "These techniques demonstrate how prompt engineering can significantly improve the usefulness and relevance of AI-generated content for specific needs.\n",
    "\n",
    "Excellent Work! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
