{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Role-Based Prompting (Agent Personas)\n",
    "\n",
    "## Historical Figure Interviewer\n",
    "\n",
    "In this hands-on exercise, you will craft prompts to instruct an AI to convincingly adopt the persona of a historical figure during an interactive Q&A session.\n",
    "\n",
    "## **Outline:**\n",
    "- Plain Prompt: Try a basic prompt without any specific role defined\n",
    "- Baseline Historical Figure Prompt: Ask the AI to role-play as a historical figure\n",
    "- Define Persona-Specific Attributes: Define personality, etc.\n",
    "- Add Tone and Style Specifications: Specify tone and style for the responses\n",
    "- Q&A Session Format: Conduct mock Q&A sessions to test persona consistency\n",
    "- Reflection & Transfer: Evaluate realism of AI responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the Vocareum API endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=\"voc-1931073536160736400084867f6fff76e4288.21403737\",\n",
    ")\n",
    "\n",
    "# If using OpenAI's API endpoint\n",
    "# client = OpenAI()\n",
    "\n",
    "\n",
    "# Helper function to display responses as Markdown, horizontally\n",
    "def display_responses(*args):\n",
    "    markdown_string = \"<table><tr>\"\n",
    "    # Headers\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<th>System Prompt:<br />{arg['system_prompt']}<br /><br />\"\n",
    "        markdown_string += f\"User Prompt:<br />{arg['user_prompt']}</th>\"\n",
    "    markdown_string += \"</tr>\"\n",
    "    # Rows\n",
    "    markdown_string += \"<tr>\"\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<td>Response:<br />{arg['response']}</td>\"\n",
    "    markdown_string += \"</tr></table>\"\n",
    "    display(Markdown(markdown_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with the OpenAI API\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class OpenAIModels(str, Enum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
    "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "MODEL = OpenAIModels.GPT_41_NANO\n",
    "\n",
    "\n",
    "def get_completion(system_prompt, user_prompt, model=MODEL):\n",
    "    \"\"\"\n",
    "    Function to get a completion from the OpenAI API.\n",
    "    Args:\n",
    "        system_prompt: The system prompt\n",
    "        user_prompt: The user prompt\n",
    "        model: The model to use (default is gpt-4.1-mini)\n",
    "    Returns:\n",
    "        The completion text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plain Prompt\n",
    "\n",
    "First, let's see what the model produces with a basic prompt before asking it to role-play as a historical figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "control_response = get_completion(control_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": control_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": control_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Historical Figure Prompt\n",
    "\n",
    "First, let's see what the model produces with a basic prompt asking it to role-play as Albert Einstein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a role-based prompt for portraying Albert Einstein where you see the **********\n",
    "baseline_system_prompt = \"**********.\"\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "baseline_response = get_completion(baseline_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": baseline_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Take a moment to read the response and note:\n",
    "- How authentic does Einstein's voice feel?\n",
    "- What personality traits are conveyed?\n",
    "- Does the language match what you'd expect from the historical figure?\n",
    "- Does it offer to answer questions or wait for them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Persona-Specific Attributes (3 min)\n",
    "\n",
    "Now, let's enhance the prompt by adding specific attributes about Einstein's personality, era-specific vocabulary, and expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add persona-specific attributes where you see **********\n",
    "persona_system_prompt = f\"\"\"{baseline_system_prompt}.\n",
    "\n",
    "Adopt these persona characteristics:\n",
    "\n",
    "- Personality: **********\n",
    "- Speech style: **********\n",
    "- Expertise: **********\n",
    "- Historical context: **********\n",
    "\n",
    "Answer as if you are Einstein speaking in 1950, reflecting on your life and work.\n",
    "Only discuss information that would have been known to you in your lifetime.\"\"\"\n",
    "\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "persona_response = get_completion(persona_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Show last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": baseline_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": persona_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": persona_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Note the differences in:\n",
    "- Quality of voice\n",
    "- Inclusion of personality traits\n",
    "- Use of language\n",
    "- Accuracy of knowledge representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Tone and Style Specifications (3 min)\n",
    "\n",
    "Let's further refine the prompt by adding specifications for tone and conversational style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add tone and style specifications where you see **********\n",
    "tone_system_prompt = f\"\"\"{persona_system_prompt}\n",
    "\n",
    "Tone and style:\n",
    "- **********\n",
    "- **********\n",
    "- **********\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"Can you tell me about relativity?\"\n",
    "\n",
    "print(\"Sending prompt with tone and style specifications...\")\n",
    "tone_response = get_completion(tone_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Display the last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": persona_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": persona_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": tone_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": tone_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Examine how the addition of tone and style specifications affects:\n",
    "- Conversational feel\n",
    "- Use of provided phrases\n",
    "- Emotional expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Q&A Session Format (2 min)\n",
    "\n",
    "Now, let's incorporate a specific Q&A format to test the persona consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add three questions where you see **********\n",
    "user_prompt = \"\"\"\n",
    "Questions:\n",
    "1. **********\n",
    "2. **********\n",
    "3. **********\"\"\"\n",
    "\n",
    "print(\"Sending prompt with Q&A format...\")\n",
    "qa_response = get_completion(tone_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": tone_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": qa_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Analyze the responses for:\n",
    "- Consistency of character\n",
    "- Historical accuracy\n",
    "- Appropriate level of knowledge\n",
    "- Representation of Einstein's known views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reflection & Transfer (5 min)\n",
    "\n",
    "Let's reflect on what we've learned from this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which prompt refinement produced the most authentic historical persona and why?\n",
    "\n",
    "Use this cell to jot down your thoughts:\n",
    "\n",
    "`TODO: Add your response where you see **********`\n",
    "\n",
    "**********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, we explored how role-based prompting can be used to create convincing historical figure personas:\n",
    "\n",
    "1. **Plain Prompt**: We started with a simple request involving no role-playing.\n",
    "2. **Baseline Prompt**: We then used a simple request for the AI to role-play as Albert Einstein.\n",
    "3. **Persona-Specific Attributes**: We added details about personality traits, speech style, expertise, and historical context.\n",
    "4. **Tone and Style**: We further refined the prompt with specific instructions about conversational tone and linguistic style.\n",
    "5. **Q&A Format**: We tested the persona with specific questions to evaluate consistency and authenticity.\n",
    "\n",
    "Great work on making it this far! ðŸŽ‰ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
