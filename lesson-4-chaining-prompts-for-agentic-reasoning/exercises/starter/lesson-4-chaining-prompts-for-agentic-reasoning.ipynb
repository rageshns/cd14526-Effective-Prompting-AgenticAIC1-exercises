{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 4: Chaining Prompts for Agentic Reasoning\n",
        "\n",
        "## Automated Claim Triage: From First-Notice to the Right Queue\n",
        "\n",
        "In this hands-on exercise, you will build a prompt chain that extracts key fields from free-form auto-claim reports, assesses damage severity, and routes each claim to one of several queues — `glass`, `fast_track`, `material_damage`, or `total_loss` — with code-based gate checks at every step.\n",
        "\n",
        "## Outline:\n",
        "\n",
        "- Setup\n",
        "- Sample FNOL (First Notice of Loss) Texts\n",
        "- Stage I: Information Extraction\n",
        "- Stage II: Severity Assessment\n",
        "- Stage III: Queue Routing\n",
        "- Review Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary libraries and define helper functions, including a mock LLM client, code execution environment, and test runner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "# No changes needed in this cell\n",
        "from openai import OpenAI  # For accessing the OpenAI API\n",
        "from enum import Enum\n",
        "import json\n",
        "from pydantic import BaseModel, Field  # For structured data validation\n",
        "from typing import List, Literal, Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up LLM credentials\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openai.vocareum.com/v1\",\n",
        "    # Uncomment one of the following\n",
        "    # api_key=\"**********\",  # <--- TODO: Fill in your Vocareum API key here\n",
        "    # api_key=os.getenv(\n",
        "    #     \"OPENAI_API_KEY\"\n",
        "    # ),  # <-- Alternately, set as an environment variable\n",
        ")\n",
        "\n",
        "# If using OpenAI's API endpoint\n",
        "# client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define helper functions\n",
        "# No changes needed in this cell\n",
        "\n",
        "\n",
        "class OpenAIModels(str, Enum):\n",
        "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
        "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
        "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
        "\n",
        "\n",
        "MODEL = OpenAIModels.GPT_41_NANO\n",
        "\n",
        "\n",
        "def get_completion(messages=None, system_prompt=None, user_prompt=None, model=MODEL):\n",
        "    \"\"\"\n",
        "    Function to get a completion from the OpenAI API.\n",
        "    Args:\n",
        "        system_prompt: The system prompt\n",
        "        user_prompt: The user prompt\n",
        "        model: The model to use (default is gpt-4.1-mini)\n",
        "    Returns:\n",
        "        The completion text\n",
        "    \"\"\"\n",
        "\n",
        "    messages = list(messages)\n",
        "    if system_prompt:\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
        "    if user_prompt:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample FNOL (First Notice of Loss) Texts\n",
        "Let's define a few sample First Notice of Loss (FNOL) texts to process through our chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sample FNOL texts\n",
        "# TODO: [Optional] Add more sample FNOL texts to test various scenarios\n",
        "\n",
        "sample_fnols = [\n",
        "    \"\"\"\n",
        "    Claim ID: C001\n",
        "    Customer: John Smith\n",
        "    Vehicle: 2018 Toyota Camry\n",
        "    Incident: While driving on the highway, a rock hit my windshield and caused a small chip\n",
        "    about the size of a quarter. No other damage was observed.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Claim ID: C002\n",
        "    Customer: Sarah Johnson\n",
        "    Vehicle: 2020 Honda Civic\n",
        "    Incident: I was parked at the grocery store and returned to find someone had hit my car and\n",
        "    dented the rear bumper and taillight. The taillight is broken and the bumper has a large dent.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Claim ID: C003\n",
        "    Customer: Michael Rodriguez\n",
        "    Vehicle: 2022 Ford F-150\n",
        "    Incident: I was involved in a serious collision at an intersection. The front of my truck is\n",
        "    severely damaged, including the hood, bumper, radiator, and engine compartment. The airbags\n",
        "    deployed and the vehicle is not drivable.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Claim ID: C004\n",
        "    Customer: Emma Williams\n",
        "    Vehicle: 2019 Subaru Outback\n",
        "    Incident: My car was damaged in a hailstorm. There are multiple dents on the hood, roof, and\n",
        "    trunk. The side mirrors were also damaged and one window has a small crack.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Claim ID: C005\n",
        "    Customer: David Brown\n",
        "    Vehicle: 2021 Tesla Model 3\n",
        "    Incident: Someone keyed my car in the parking lot. There are deep scratches along both doors\n",
        "    on the driver's side.\n",
        "    \"\"\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage I: Information Extraction\n",
        "In this stage, we'll create a prompt that extracts structured information from free-form FNOL text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a system prompt for information extraction according to the provided ClaimInformation class\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "class ClaimInformation(BaseModel):\n",
        "    claim_id: str = Field(..., min_length=2, max_length=10)\n",
        "    name: str = Field(..., min_length=2, max_length=100)\n",
        "    vehicle: str = Field(..., min_length=2, max_length=100)\n",
        "    loss_desc: str = Field(..., min_length=10, max_length=500)\n",
        "    damage_area: List[\n",
        "        Literal[\n",
        "            \"windshield\",\n",
        "            \"front\",\n",
        "            \"rear\",\n",
        "            \"side\",\n",
        "            \"roof\",\n",
        "            \"hood\",\n",
        "            \"door\",\n",
        "            \"bumper\",\n",
        "            \"fender\",\n",
        "            \"quarter panel\",\n",
        "            \"trunk\",\n",
        "            \"glass\",\n",
        "        ]\n",
        "    ] = Field(..., min_items=1)\n",
        "\n",
        "\n",
        "info_extraction_system_prompt = \"\"\"\n",
        "You are **********. Your task is to extract key information from First Notice of Loss (FNOL) reports.\n",
        "\n",
        "Format your response as a valid JSON object with the following keys:\n",
        "- claim_id (str): The claim ID\n",
        "- **********\n",
        "- **********\n",
        "- etc.\n",
        "\n",
        "********** <-- Extra instructions\n",
        "\n",
        "Only respond with the JSON object, nothing else.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a gate check function and claim extraction function\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "def gate1_validate_claim_info(claim_info_json: str) -> ClaimInformation:\n",
        "    \"\"\"\n",
        "    Gate 1: Validates claim information extracted from FNOL text.\n",
        "    Returns validated ClaimInformation object or raises validation error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the JSON string\n",
        "        claim_info_dict = json.loads(claim_info_json)\n",
        "        # Validate with Pydantic model\n",
        "        validated_info = ClaimInformation(**claim_info_dict)\n",
        "        return validated_info\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Gate 1 validation failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def extract_claim_info(fnol_text):\n",
        "    \"\"\"\n",
        "    Stage 1: Extract structured information from FNOL text\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": info_extraction_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": fnol_text},\n",
        "    ]\n",
        "\n",
        "    response = get_completion(messages=messages)\n",
        "\n",
        "    # Gate check: validate the extracted information\n",
        "    try:\n",
        "        validated_info = # ********** <-- Run the gate check on the response\n",
        "        return validated_info\n",
        "    except ValueError as e:\n",
        "        print(f\"Gate 1 failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the claim extraction function on the sample FNOLs\n",
        "# No updates needed in this cell\n",
        "\n",
        "extracted_claim_info_items = [\n",
        "    extract_claim_info(fnol_text) for fnol_text in sample_fnols\n",
        "]\n",
        "extracted_claim_info_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage II: Severity Assessment\n",
        "In this stage, we'll assess the severity of the damage based on the extracted information.\n",
        "\n",
        "Note, our carrier applies the following heuristics:\n",
        "- Minor damage: Small dents, scratches, glass chips (cost range: $100-$1,000)\n",
        "- Moderate damage: Single panel damage, bumper replacement, door damage (cost range: $1,000-$5,000)\n",
        "- Major damage: Structural damage, multiple panel replacement, engine/drivetrain issues, total loss candidates (cost range: $5,000-$50,000)\n",
        "\n",
        "In this example we will let the LLM estimate the cost, though in production we would want a more accurate estimate, e.g. querying a database of repair costs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a system prompt for severity assessment according to the provided SeverityAssessment class\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "class SeverityAssessment(BaseModel):\n",
        "    severity: Literal[\"Minor\", \"Moderate\", \"Major\"]\n",
        "    est_cost: float = Field(..., gt=0)\n",
        "\n",
        "\n",
        "severity_assessment_system_prompt = \"\"\"\n",
        "You are an auto insurance damage assessor. Your task is to evaluate the severity of vehicle damage and estimate repair costs.\n",
        "\n",
        "********** <-- Instructions\n",
        "\n",
        "Only respond with the JSON object, nothing else.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a gate check function and assess_severity function\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "def gate2_cost_range_ok(severity_json: str) -> SeverityAssessment:\n",
        "    \"\"\"\n",
        "    Gate 2: Validates that the estimated cost is within reasonable range for the severity.\n",
        "    Returns validated SeverityAssessment object or raises validation error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the JSON string\n",
        "        severity_dict = json.loads(severity_json)\n",
        "        # Validate with Pydantic model\n",
        "        validated_severity = SeverityAssessment(**severity_dict)\n",
        "\n",
        "        # Check cost range based on severity\n",
        "        if (\n",
        "            validated_severity.severity == \"Minor\"\n",
        "            and (\n",
        "                # ********** <-- est_cost outside of heuristic range for Minor will raise ValueError\n",
        "            )\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"Minor damage should cost between $100-$1000, got ${validated_severity.est_cost}\"\n",
        "            )\n",
        "        elif (\n",
        "            validated_severity.severity == \"Moderate\"\n",
        "            and (\n",
        "                # ********** <-- est_cost outside of heuristic range for Moderate will raise ValueError\n",
        "            )\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"Moderate damage should cost between $1000-$5000, got ${validated_severity.est_cost}\"\n",
        "            )\n",
        "        elif (\n",
        "            validated_severity.severity == \"Major\"\n",
        "            and (\n",
        "                # ********** <-- est_cost outside of heuristic range for Major will raise ValueError\n",
        "            )\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"Major damage should cost between $5000-$50000, got ${validated_severity.est_cost}\"\n",
        "            )\n",
        "\n",
        "\n",
        "        return validated_severity\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Gate 2 validation failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def assess_severity(claim_info: ClaimInformation) -> Optional[SeverityAssessment]:\n",
        "    \"\"\"\n",
        "    Stage 2: Assess severity based on damage description\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert Pydantic model to JSON string\n",
        "    claim_info_json = claim_info.model_dump_json()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": severity_assessment_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": claim_info_json},\n",
        "    ]\n",
        "\n",
        "    response = get_completion(messages=messages)\n",
        "\n",
        "    # Gate check: validate the severity assessment\n",
        "    try:\n",
        "        validated_severity = gate2_cost_range_ok(response)\n",
        "        return validated_severity\n",
        "    except ValueError as e:\n",
        "        print(f\"Gate 2 failed: {e}. Response: {response}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the claim extraction function on the sample data\n",
        "# No updates needed in this cell\n",
        "\n",
        "severity_assessment_items = [\n",
        "    assess_severity(item) for item in extracted_claim_info_items\n",
        "]\n",
        "\n",
        "severity_assessment_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Stage III: Queue Routing\n",
        "In this stage, we'll route the claim to the appropriate queue based on severity and damage area.\n",
        "\n",
        "Use these routing rules:\n",
        "- 'glass' queue: For Minor damage involving ONLY glass (windshield, windows)\n",
        "- 'fast_track' queue: For other Minor damage\n",
        "- 'material_damage' queue: For all Moderate damage\n",
        "- 'total_loss' queue: For all Major damage\n",
        "\n",
        "These are the priority levels:\n",
        "- Priority 1 (highest): Safety issues, customer stranded\n",
        "- Priority 2: Significant but contained damage, vehicle drivable\n",
        "- Priority 3: Standard claims\n",
        "- Priority 4: Minor issues, no mobility impact\n",
        "- Priority 5 (lowest): Cosmetic only, no functional impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a system prompt for claim routing according to the provided ClaimRouting class\n",
        "# TODO: Complete the prompt by replacing the parts marked with **********\n",
        "\n",
        "\n",
        "class ClaimRouting(BaseModel):\n",
        "    claim_id: str\n",
        "    queue: Literal[\"glass\", \"fast_track\", \"material_damage\", \"total_loss\"]\n",
        "    priority: int = Field(..., ge=1, le=5)\n",
        "\n",
        "\n",
        "queue_routing_system_prompt = \"\"\"\n",
        "You are an auto insurance claim routing specialist. Your task is to determine the appropriate processing queue for each claim.\n",
        "\n",
        "********** <-- Instructions\n",
        "\n",
        "Only respond with the JSON object, nothing else.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a gate check function and assess_severity function\n",
        "# No updates needed in this cell\n",
        "\n",
        "\n",
        "def gate3_validate_routing(routing_json: str) -> ClaimRouting:\n",
        "    \"\"\"\n",
        "    Gate 3: Validates that the claim is routed to a valid queue.\n",
        "    Returns validated ClaimRouting object or raises validation error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the JSON string\n",
        "        routing_dict = json.loads(routing_json)\n",
        "        # Validate with Pydantic model\n",
        "        validated_routing = ClaimRouting(**routing_dict)\n",
        "        return validated_routing\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Gate 3 validation failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def route_claim(\n",
        "    claim_info: ClaimInformation, severity_assessment: Optional[SeverityAssessment]\n",
        ") -> Optional[ClaimRouting]:\n",
        "    \"\"\"\n",
        "    Stage 3: Route claim to appropriate queue\n",
        "    \"\"\"\n",
        "    if severity_assessment is None:\n",
        "        return None\n",
        "\n",
        "    # Create input for the routing model\n",
        "    routing_input = {\n",
        "        \"claim_info\": claim_info.model_dump(),\n",
        "        \"severity_assessment\": severity_assessment.model_dump(),\n",
        "    }\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": queue_routing_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": json.dumps(routing_input)},\n",
        "    ]\n",
        "\n",
        "    response = get_completion(messages=messages)\n",
        "\n",
        "    # Gate check: validate the routing decision\n",
        "    try:\n",
        "        validated_routing = gate3_validate_routing(response)\n",
        "        return validated_routing\n",
        "    except ValueError as e:\n",
        "        print(f\"Gate 3 failed: {e}. Response: {response}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the routing function on the sample data\n",
        "# No updates needed in this cell\n",
        "\n",
        "routed_claim_items = [\n",
        "    route_claim(claim, severity_assessment)\n",
        "    for claim, severity_assessment in zip(\n",
        "        extracted_claim_info_items, severity_assessment_items\n",
        "    )\n",
        "]\n",
        "\n",
        "routed_claim_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Review Outputs\n",
        "\n",
        "Let's put our data into a pandas dataframe for easier analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# No updates needed in this cell\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "records = []\n",
        "for claim, severity_assessment, routed_claim in zip(\n",
        "    extracted_claim_info_items, severity_assessment_items, routed_claim_items\n",
        "):\n",
        "    record = {}\n",
        "    record.update(claim)\n",
        "    record.update(severity_assessment)\n",
        "    record.update(routed_claim)\n",
        "    records.append(record)\n",
        "\n",
        "\n",
        "# Show the entire dataframe since it is not too large\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Reflection & Transfer\n",
        "Reflect on the effectiveness of chaining prompts for this task:\n",
        "\n",
        "1. Prompt Chain Architecture:\n",
        "* How does breaking down the task into stages affect the performance?\n",
        "* What are the benefits of having gate checks between stages?\n",
        "* How could the chain design be improved?\n",
        "\n",
        "2. Error Handling:\n",
        "* What types of errors might occur at each stage?\n",
        "* How could we make the chain more robust against errors?\n",
        "* What would a good fallback strategy look like?\n",
        "\n",
        "3. Scalability:\n",
        "* How well would this approach scale to handle more complex claims?\n",
        "* What challenges might arise when processing thousands of claims?\n",
        "* How could the prompt chain be optimized for efficiency?\n",
        "\n",
        "4. Transfer to Other Domains:\n",
        "* How could this prompt chaining approach be applied to other domains?\n",
        "* What general principles of prompt chaining can we extract from this exercise?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "🎉 Congratulations! 🎉 You've built an impressive prompt chain system for insurance claims!\n",
        "You transformed messy FNOL text into structured data, assessed damage severity, and routed claims to the right queues, all with robust gate checks! 🚀✨\n",
        "\n",
        "Remember:\n",
        "\n",
        "- 🔗 Chained prompts break complex tasks into manageable steps\n",
        "- 🛡️ Gate checks prevent error cascades\n",
        "- 🧠 Having specialized prompts helps keep code focused and maintainable\n",
        "\n",
        "You've mastered a powerful pattern for countless business processes! 🏆\n",
        "Amazing work on your agentic reasoning system! 💯"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
